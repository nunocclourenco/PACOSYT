{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from em import COLUMNS_2P, compute_all_lq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads the ALL SRIs to a pandas dataframe and saves CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_a=['ID', 'Nt', 'Din', \"W\"]\n",
    "names_z=['SRF']\n",
    "names_t=['ID', 'Nt', 'Din', 'W', 'SRF']\n",
    "\n",
    "\n",
    "def line_split(line):\n",
    "    return re.findall(r'[^\"\\s]\\S*|\".+?\"', line)\n",
    "\n",
    "def load(zip_file, folder, manifest_fname=\"manifest\", names=names_t):\n",
    "    \n",
    "    df = pd.read_csv( zip_file.open(manifest_fname), \n",
    "                     sep=\" \",index_col=0,\n",
    "                     names=names, engine=\"python\")\n",
    "\n",
    "    points = {'ID':[]}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        freq_lines = []\n",
    "        try:\n",
    "            with zip_file.open(folder + \"/\" + str(int(index),) +\".sri\") as fi1e:\n",
    "                freq_idx = 0;\n",
    "                points['ID'].append(index)\n",
    "                for line in fi1e:\n",
    "                    line = line.decode().strip()\n",
    "                    if line.startswith('#') or line.startswith('!'): continue \n",
    "                    token = line.split()\n",
    "                    if len(token) != len(COLUMNS_2P): print(\"Invalid header!\") \n",
    "                    \n",
    "                    values = [float(x) for x in token]\n",
    "                    \n",
    "                    for i in range(len(COLUMNS_2P)):\n",
    "                        key = COLUMNS_2P[i]+\"_\"+str(freq_idx)\n",
    "                        if key not in points: points[key] = []\n",
    "                        points[key].append(values[i])\n",
    "                    freq_idx = freq_idx + 1\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    return (pd.concat((df, pd.DataFrame(points).set_index(\"ID\")), axis=1 ), freq_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"There is no item named 'inductGR_350nm_200GHz/indGR_train/321.sri' in the archive\"\n"
     ]
    }
   ],
   "source": [
    "# One file per turn per frequency\n",
    "save_data = True\n",
    "\n",
    "data_folder = '../data/inductGR_350nm_200GHz/'\n",
    "data_file = '../data/inductGR_350nm_200GHz.zip'\n",
    "\n",
    "if os.path.exists(data_folder): \n",
    "    shutil.rmtree(data_folder)\n",
    "os.mkdir(data_folder)\n",
    "\n",
    "zf_data = zipfile.ZipFile(data_file,  mode=\"r\")\n",
    "\n",
    "\n",
    "ind_TEST, freq_points_test = load(zf_data, f\"inductGR_350nm_200GHz/indGR_test\",\n",
    "                manifest_fname= f\"inductGR_350nm_200GHz/input_samples_test.in\",\n",
    "                names = names_a)\n",
    "\n",
    "\n",
    "ind_TRAIN, freq_points_train = load(zf_data, f\"inductGR_350nm_200GHz/indGR_train\",\n",
    "                manifest_fname= f\"inductGR_350nm_200GHz/input_samples_training.in\",\n",
    "                names = names_a)\n",
    "\n",
    "ind_TEST['SRF'] = [20e9]*len(ind_TEST) \n",
    "ind_TRAIN['SRF'] = [200e9]*len(ind_TRAIN) \n",
    "\n",
    "ind_TRAIN.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lq_test = [None for i in range(freq_points_test)]\n",
    "lq_train = [None for i in range(freq_points_train)]\n",
    "\n",
    "\n",
    "for i, f in enumerate(range(freq_points_test)):\n",
    "    c = [col+\"_\"+str(f) for col in COLUMNS_2P] \n",
    "    lq_test[i] =  compute_all_lq(ind_TEST[c[0]].values,ind_TEST[c[1:]].values)\n",
    "\n",
    "for i, f in enumerate(range(freq_points_train)):\n",
    "    c = [col+\"_\"+str(f) for col in COLUMNS_2P] \n",
    "    lq_train[i] =  compute_all_lq(ind_TRAIN[c[0]].values,ind_TRAIN[c[1:]].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, (index, row) in enumerate(ind_TEST.iterrows()):\n",
    "    l_prev = lq_test[0][r,0]\n",
    "    for i, f in enumerate(range(freq_points_test)):\n",
    "        l_curr = lq_test[i][r,0]\n",
    "        if l_prev > 0 and l_curr < 0:\n",
    "            ind_TEST.loc[index, 'SRF'] = (ind_TEST.loc[index, 'freq_'+str(i)] + ind_TEST.loc[index, 'freq_'+str(i-1)])/2\n",
    "            break\n",
    "\n",
    "for r, (index, row) in enumerate(ind_TRAIN.iterrows()):\n",
    "    l_prev = lq_train[0][r,0]\n",
    "    \n",
    "    for i, f in enumerate(range(freq_points_train)):\n",
    "        l_curr = lq_train[i][r,0]\n",
    "        if l_prev > 0 and l_curr < 0:\n",
    "            ind_TRAIN.loc[index, 'SRF'] = (ind_TRAIN.loc[index, 'freq_'+str(i)] + ind_TRAIN.loc[index, 'freq_'+str(i-1)])/2\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = [[] for i in range(freq_points_train)]\n",
    "dfs_test = [[] for i in range(freq_points_test)]\n",
    "\n",
    "\n",
    "for n in range(5):\n",
    "\n",
    "    ind_nturn_TRAIN = ind_TRAIN[ind_TRAIN.Nt == (n+1)]\n",
    "    ind_nturn_TEST = ind_TEST[ind_TEST.Nt == (n+1)] \n",
    "\n",
    "    zf_test = zipfile.ZipFile(data_folder + f'test_dataset_{n+1}T.csv.zip',  mode=\"w\", compression=zipfile.ZIP_DEFLATED)\n",
    "    zf_train = zipfile.ZipFile(data_folder + f'train_dataset_{n+1}T.csv.zip',  mode=\"w\", compression=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "\n",
    "    for i, f in enumerate(range(freq_points_train)):\n",
    "        c = names_t[1:] + [col+\"_\"+str(f) for col in COLUMNS_2P] \n",
    "        \n",
    "        transf_TRAIN_2 = ind_nturn_TRAIN[c]\n",
    "        if(i < freq_points_test): \n",
    "            transf_TEST_2 = ind_nturn_TEST[c]\n",
    "\n",
    "        rm = {}\n",
    "        for col in COLUMNS_2P:\n",
    "            rm[col+\"_\"+str(f)] = col\n",
    "        \n",
    "\n",
    "        transf_TRAIN_2 = transf_TRAIN_2.rename(columns=rm)\n",
    "\n",
    "        \n",
    "        dfs_train[i].append(transf_TRAIN_2)\n",
    "        \n",
    "        if(i < freq_points_test): \n",
    "            transf_TEST_2 = transf_TEST_2.rename(columns=rm)\n",
    "            dfs_test[i].append(transf_TEST_2)\n",
    "\n",
    "        if save_data :\n",
    "            if(i < freq_points_test): zf_test.writestr(\"test_dataset_\"+str(f)+\".csv\", transf_TEST_2.to_csv())\n",
    "            zf_train.writestr(\"training_dataset_\"+str(f)+\".csv\", transf_TRAIN_2.to_csv())\n",
    "\n",
    "    if save_data :\n",
    "        zf_test.close()\n",
    "        zf_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One file per frequency\n",
    "save_data = True\n",
    "\n",
    "if save_data :\n",
    "    zf_test = zipfile.ZipFile(data_folder + f'test_dataset_allT.csv.zip',  mode=\"w\", compression=zipfile.ZIP_DEFLATED)\n",
    "    zf_train = zipfile.ZipFile(data_folder + f'train_dataset_allT.csv.zip',  mode=\"w\", compression=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "df_allT_train = [None]*freq_points_train\n",
    "df_allT_test = [None]*freq_points_test\n",
    "\n",
    "for i, f in enumerate(range(freq_points_train)):\n",
    "    df_allT_train[i] = pd.concat(dfs_train[i],keys=range(1,6))\n",
    "    if save_data :\n",
    "        zf_train.writestr(\"training_dataset_\"+str(f)+\".csv\", df_allT_train[i].to_csv())\n",
    "\n",
    "    if(i < freq_points_test):\n",
    "        df_allT_test[i] = pd.concat(dfs_test[i],keys=range(1,9))\n",
    "        if save_data :\n",
    "            zf_test.writestr(\"test_dataset_\"+str(f)+\".csv\", df_allT_test[i] .to_csv())\n",
    "\n",
    "if save_data :\n",
    "    zf_test.close()\n",
    "    zf_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One with all \n",
    "df_allFT = pd.concat(df_allT_train,keys=range(freq_points_train))\n",
    "df_allFT.to_csv(data_folder + 'train_dataset_allTF.csv.zip')\n",
    "\n",
    "\n",
    "df_allFT = pd.concat(df_allT_test,keys=range(freq_points_test))\n",
    "df_allFT.to_csv(data_folder + 'test_dataset_allTF.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Copyright (C) 2022 Instituto de TelecomunicaÃ§Ãµes & IMSE CSIC"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e5760e712c30506cacfbb8d55a91dfa6d917c7cae71b450333e4dc838792f2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
